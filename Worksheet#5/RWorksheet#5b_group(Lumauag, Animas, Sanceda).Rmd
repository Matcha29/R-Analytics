---
title: "RWorksheet#5b_group(Lumauag, Animas, Sanceda)"
author: "Matt Andrei Lumauag"
date: "2024-12-02"
output: pdf_document
---

```{r}
library(rvest)      # For scraping web data
library(dplyr)      # For manipulating and cleaning data
library(stringr)    # For string manipulation
```


```{r}
# Define product links and product names
product_links <- c(
  "https://www.amazon.com/Fossil-Minimalist-Quartz-Stainless-Leather/dp/B079DD13WB/ref=sr_1_3",
  "https://www.amazon.com/Fossil-Quartz-Stainless-Steel-Chronograph/dp/B008AXYWHQ/ref=sr_1_5",
  "https://www.amazon.com/Timex-T2H281-Reader-Black-Leather/dp/B000AYYIYU/ref=sr_1_7",
  "https://www.amazon.com/Amazon-Essentials-Silver-Tone-Black-Strap/dp/B07YQFST6X/ref=sr_1_11",
  "https://www.amazon.com/Citizen-Mens-Quartz-Watch-BF0580-57L/dp/B008FG6ZRI/ref=sr_1_9",
  "https://www.amazon.com/Timex-Mens-Expedition-Scout-Watch/dp/B09RTQGH4W/ref=sr_1_6",
  "https://www.amazon.com/Michael-Kors-Runway-Black-MK8507/dp/B01HEVAPSO/ref=sr_1_12",
  "https://www.amazon.com/Diesel-Chief-Quartz-Stainless-Casual/dp/B004C5ZVLM/ref=sr_1_14",
  "https://www.amazon.com/Casio-F108WH-Illuminator-Collection-Digital/dp/B0053HBJBE/ref=sr_1_13",
  "https://www.amazon.com/GOLDEN-HOUR-Stainless-Waterproof-Chronograph/dp/B087JG6H25/ref=sr_1_15"
)

product_names <- c(
  "Fossil Minimalist Men's Watch with Leather or Stainless Steel Band",
  "Fossil Nate Men's Watch with Oversized Chronograph Watch Dial and Stainless Steel or Leather Band",
  "Timex Men's Easy Reader Watch",
  "Amazon Essentials Men's Easy to Read Strap Watch",
  "Citizen Men's Classic Quartz Watch, Stainless Steel",
  "Timex Men's Expedition Scout 40mm Watch",
  "Michael Kors Oversized Slim Runway Men's Watch, Stainless Steel Watch for Men",
  "Diesel Master Chief Stainless Steel Chronograph Men's Watch",
  "Casio F108WH Series",
  "GOLDEN HOUR Fashion Business Mens Watches"
)

category <- "Watches"
```


```{r}
# Initialize an empty dataframe
all_reviews <- data.frame()
```


```{r}
# Loop through each product
for (i in seq_along(product_links)) {
  url <- product_links[i]
  product_name <- product_names[i]
  
  # Variable to store all reviews for the current product
  reviewer_names <- character()
  review_dates <- character()
  review_titles <- character()
  review_comments <- character()
  verified_purchases <- character()
  star_ratings <- numeric()
  
  # Loop to scrape multiple pages (each page has 10 reviews)
  page_num <- 1
  while(length(reviewer_names) < 20) {
    # Modify the URL to include the page number for pagination
    paginated_url <- paste0(url, "?pageNumber=", page_num)
    
    try({
      webpage <- read_html(paginated_url)
      
      # Extract review sections
      reviews <- webpage %>%
        html_nodes(".review")
      
      # Extract reviewer names
      reviewer_names_page <- reviews %>%
        html_nodes(".a-profile-name") %>%
        html_text(trim = TRUE)
      
      # Extract review dates
      review_dates_page <- reviews %>%
        html_nodes(".review-date") %>%
        html_text(trim = TRUE)
      
      # Extract review titles
      review_titles_page <- reviews %>%
        html_nodes(".review-title span") %>%
        html_text(trim = TRUE)
      
      # Extract review comments
      review_comments_page <- reviews %>%
        html_nodes(".review-text-content span") %>%
        html_text(trim = TRUE)
      
      # Extract verified purchase labels
      verified_purchases_page <- reviews %>%
        html_nodes(".review-vp-label") %>%
        html_text(trim = TRUE)
      
      # Extract star ratings
      star_ratings_page <- reviews %>%
        html_nodes(".a-icon-alt") %>%
        html_text(trim = TRUE) %>%
        str_extract("\\d\\.\\d") %>%  # Extract the numeric rating
        as.numeric()
      
      # Append the data
      reviewer_names <- c(reviewer_names, reviewer_names_page)
      review_dates <- c(review_dates, review_dates_page)
      review_titles <- c(review_titles, review_titles_page)
      review_comments <- c(review_comments, review_comments_page)
      verified_purchases <- c(verified_purchases, verified_purchases_page)
      star_ratings <- c(star_ratings, star_ratings_page)
      
      # Increment page number to move to the next page
      page_num <- page_num + 1
    }, silent = TRUE)
    
    # If we've collected enough reviews, break out of the loop
    if(length(reviewer_names) >= 20) {
      break
    }
  }
  
  # Limit to the first 20 reviews
  max_reviews <- min(20, length(reviewer_names))
  reviewer_names <- reviewer_names[1:max_reviews]
  review_dates <- review_dates[1:max_reviews]
  review_titles <- review_titles[1:max_reviews]
  review_comments <- review_comments[1:max_reviews]
  verified_purchases <- verified_purchases[1:max_reviews]
  star_ratings <- star_ratings[1:max_reviews]
  
  # Create a dataframe for this product
  review_data <- data.frame(
    Category = rep(category, max_reviews),
    ProductName = rep(product_name, max_reviews),
    Reviewer = reviewer_names,
    Date = review_dates,
    Title = review_titles,
    Comment = review_comments,
    StarRating = star_ratings,
    VerifiedPurchase = verified_purchases,
    stringsAsFactors = FALSE
  )
  
  # Append to the main dataframe
  all_reviews <- bind_rows(all_reviews, review_data)
}

# Print the result
print(all_reviews)
```

```{r}
# Save the reviews to a CSV file
write.csv(all_reviews, "Watch_Reviews.csv", row.names = FALSE)
```